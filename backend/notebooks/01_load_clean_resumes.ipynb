{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40a4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the resumes\n",
    "resumes = pd.read_csv('../data/raw/UpdatedResumeDataSet.csv')\n",
    "\n",
    "# Load the job descriptions\n",
    "jobs = pd.read_csv('../data/raw/job_title_des.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60770c",
   "metadata": {},
   "source": [
    "Check the quality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90fe9b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2277, 3)\n",
      "Index(['Unnamed: 0', 'Job Title', 'Job Description'], dtype='object')\n",
      "0\n",
      "0\n",
      "Flutter Developer\n",
      "We are looking for hire experts flutter developer. So you are eligible this post then apply your resume.\n",
      "Job Types: Full-time, Part-time\n",
      "Salary: ₹20,000.00 - ₹40,000.00 per month\n",
      "Benefits:\n",
      "Flexible schedule\n",
      "Food allowance\n",
      "Schedule:\n",
      "Day shift\n",
      "Supplemental Pay:\n",
      "Joining bonus\n",
      "Overtime pay\n",
      "Experience:\n",
      "total work: 1 year (Preferred)\n",
      "Housing rent subsidy:\n",
      "Yes\n",
      "Industry:\n",
      "Software Development\n",
      "Work Remotely:\n",
      "Temporarily due to COVID-19\n"
     ]
    }
   ],
   "source": [
    "print(jobs.shape)\n",
    "print(jobs.columns)\n",
    "\n",
    "# Drop the unnamed column\n",
    "jobs = jobs.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Rename columns for easier access\n",
    "jobs = jobs.rename(columns={\n",
    "        'Job Title': 'job_title',\n",
    "        'Job Description': 'job_description'\n",
    "})\n",
    "\n",
    "# Check for missing values\n",
    "print(jobs['job_title'].isnull().sum())\n",
    "print(jobs['job_description'].isnull().sum())\n",
    "\n",
    "\n",
    " # Inspect \n",
    "print(jobs['job_title'][0])\n",
    "print(jobs['job_description'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2950fa",
   "metadata": {},
   "source": [
    "Check out some more job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca5d392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flutter Developer\n",
      "We are looking for hire experts flutter developer. So you are eligible this post then apply your resume.\n",
      "Job Types: Full-time, Part-time\n",
      "Salary: ₹20,000.00 - ₹40,000.00 per month\n",
      "Benefits:\n",
      "Flexible schedule\n",
      "Food allowance\n",
      "Schedule:\n",
      "Day shift\n",
      "Supplemental Pay:\n",
      "Joining bonus\n",
      "Overtime pay\n",
      "Experience:\n",
      "total work: 1 year (Preferred)\n",
      "Housing rent subsidy:\n",
      "Yes\n",
      "Industry:\n",
      "Software Development\n",
      "Work Remotely:\n",
      "Temporarily due to COVID-19\n",
      "\n",
      "\n",
      "Django Developer\n",
      "PYTHON/DJANGO (Developer/Lead) - Job Code(PDJ - 04)\n",
      "Strong Python experience in API development (REST/RPC).\n",
      "Experience working with API Frameworks (Django/flask).\n",
      "Experience evaluating and improving the efficiency of programs in a Linux environment.\n",
      "Ability to effectively handle multiple tasks with a high level of accuracy and attention to detail.\n",
      "Good verbal and written communication skills.\n",
      "Working knowledge of SQL.\n",
      "JSON experience preferred.\n",
      "Good knowledge in automated unit testing using PyUnit.\n",
      "\n",
      "\n",
      "Machine Learning\n",
      "Data Scientist (Contractor)\n",
      "\n",
      "Bangalore, IN\n",
      "\n",
      "Responsibilities\n",
      "\n",
      "We are looking for a capable data scientist to join the Analytics team, reporting locally in India Bangalore. This person’s responsibilities include research, design and development of Machine Learning and Deep Learning algorithms to tackle a variety of Fraud oriented challenges. The data scientist will work closely with software engineers and program managers to deliver end-to-end products, including: data collection in big scale and analysis, exploring different algorithmic approaches, model development, assessment and validation – all the way through production.\n",
      "\n",
      "Qualifications\n",
      "\n",
      "At least 3 years of hands-on development of complex Machine Learning models using modern frameworks and tools, ideally Python based.\n",
      "Solid understanding of statistics and applied mathematics\n",
      "Creative thinker with a proven ability to tackle open problems and apply non-trivial solutions.\n",
      "Experience in software development using Python, Java or a similar language.\n",
      "Any Graduate or M.Sc. in Computer Science, Mathematics or equivalent, preferably in Machine Learning\n",
      "Ability to write clean and concise code\n",
      "Quick learner, independent, methodical, and detail oriented.\n",
      "Team player, positive attitude, collaborative, good communication skills.\n",
      "Dedicated, makes things happen.\n",
      "Flexible, capable of making decisions in an ambiguous and changing environment.\n",
      "\n",
      "Advantages:\n",
      "\n",
      "Prior experience as a software developer or data engineer – advantage\n",
      "Experience with Big data – advantage\n",
      "Experience with Spark – big advantage\n",
      "Experience with Deep Learning frameworks (PyTorch, TensorFlow, Keras) – advantage.\n",
      "Experience in the Telecommunication domain and/or Fraud prevention - advantage\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "        print(jobs['job_title'][i])\n",
    "        print(jobs['job_description'][i])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7002daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(962, 2)\n",
      "Index(['Category', 'Resume'], dtype='object')\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(resumes.shape)\n",
    "print(resumes.columns)\n",
    "\n",
    "resumes = resumes.rename(columns={\n",
    "        'Category': 'category',\n",
    "        'Resume': 'resume'\n",
    "})\n",
    "\n",
    "# print(resumes.columns)\n",
    "\n",
    "# Check for missing values\n",
    "print(resumes['category'].isnull().sum())\n",
    "print(resumes['resume'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24101e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science\n",
      "Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details \n",
      "\n",
      "Data Science Assurance Associate \n",
      "\n",
      "Data Science Assurance Associate - Ernst & Young LLP\n",
      "Skill Details \n",
      "JAVASCRIPT- Exprience - 24 months\n",
      "jQuery- Exprience - 24 months\n",
      "Python- Exprience - 24 monthsCompany Details \n",
      "company - Ernst & Young LLP\n",
      "description - Fraud Investigations and Dispute Services   Assurance\n",
      "TECHNOLOGY ASSISTED REVIEW\n",
      "TAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\n",
      "* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\n",
      "* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.\n",
      "* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\n",
      "\n",
      "Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\n",
      "\n",
      "MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\n",
      "TEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.\n",
      "* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.\n",
      "* Created customized tableau dashboards for effective reporting and visualizations.\n",
      "CHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.\n",
      "* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.\n",
      "* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.\n",
      "\n",
      "Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\n",
      "\n",
      "INFORMATION GOVERNANCE\n",
      "Organizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.\n",
      "* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\n",
      "* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.\n",
      "* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.\n",
      "Tools & Technologies: Python, Flask, Elastic Search, Kibana\n",
      "\n",
      "FRAUD ANALYTIC PLATFORM\n",
      "Fraud Analytics and investigative platform to review all red flag cases.\n",
      "â¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.\n",
      "* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\n",
      "Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js\n",
      "\n",
      "\n",
      "Data Science\n",
      "Education Details \n",
      "May 2013 to May 2017 B.E   UIT-RGPV\n",
      "Data Scientist \n",
      "\n",
      "Data Scientist - Matelabs\n",
      "Skill Details \n",
      "Python- Exprience - Less than 1 year months\n",
      "Statsmodels- Exprience - 12 months\n",
      "AWS- Exprience - Less than 1 year months\n",
      "Machine learning- Exprience - Less than 1 year months\n",
      "Sklearn- Exprience - Less than 1 year months\n",
      "Scipy- Exprience - Less than 1 year months\n",
      "Keras- Exprience - Less than 1 year monthsCompany Details \n",
      "company - Matelabs\n",
      "description - ML Platform for business professionals, dummies and enthusiasts.\n",
      "60/A Koramangala 5th block,\n",
      "Achievements/Tasks behind sukh sagar, Bengaluru,\n",
      "India                               Developed and deployed auto preprocessing steps of machine learning mainly missing value\n",
      "treatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction.\n",
      "Deployed automated classification and regression model.\n",
      "linkedin.com/in/aditya-rathore-\n",
      "b4600b146                           Reasearch and deployed the time series forecasting model ARIMA, SARIMAX, Holt-winter and\n",
      "Prophet.\n",
      "Worked on meta-feature extracting problem.\n",
      "github.com/rathorology\n",
      "Implemented a state of the art research paper on outlier detection for mixed attributes.\n",
      "company - Matelabs\n",
      "description - \n",
      "\n",
      "\n",
      "Data Science\n",
      "Areas of Interest Deep Learning, Control System Design, Programming in-Python, Electric Machinery, Web Development, Analytics Technical Activities q Hindustan Aeronautics Limited, Bangalore - For 4 weeks under the guidance of Mr. Satish, Senior Engineer in the hangar of Mirage 2000 fighter aircraft Technical Skills Programming Matlab, Python and Java, LabView, Python WebFrameWork-Django, Flask, LTSPICE-intermediate Languages and and MIPOWER-intermediate, Github (GitBash), Jupyter Notebook, Xampp, MySQL-Basics, Python Software Packages Interpreters-Anaconda, Python2, Python3, Pycharm, Java IDE-Eclipse Operating Systems Windows, Ubuntu, Debian-Kali Linux Education Details \n",
      "January 2019 B.Tech. Electrical and Electronics Engineering  Manipal Institute of Technology\n",
      "January 2015    DEEKSHA CENTER\n",
      "January 2013    Little Flower Public School\n",
      "August 2000    Manipal Academy of Higher\n",
      "DATA SCIENCE \n",
      "\n",
      "DATA SCIENCE AND ELECTRICAL ENTHUSIAST\n",
      "Skill Details \n",
      "Data Analysis- Exprience - Less than 1 year months\n",
      "excel- Exprience - Less than 1 year months\n",
      "Machine Learning- Exprience - Less than 1 year months\n",
      "mathematics- Exprience - Less than 1 year months\n",
      "Python- Exprience - Less than 1 year months\n",
      "Matlab- Exprience - Less than 1 year months\n",
      "Electrical Engineering- Exprience - Less than 1 year months\n",
      "Sql- Exprience - Less than 1 year monthsCompany Details \n",
      "company - THEMATHCOMPANY\n",
      "description - I am currently working with a Casino based operator(name not to be disclosed) in Macau.I need to segment the customers who visit their property based on the value the patrons bring into the company.Basically prove that the segmentation can be done in much better way than the current system which they have with proper numbers to back it up.Henceforth they can implement target marketing strategy to attract their customers who add value to the business.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "        print(resumes['category'][i])\n",
    "        print(resumes['resume'][i])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf0e11",
   "metadata": {},
   "source": [
    "Clean the resumes and descriptions for more consistent formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822f9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "        text = re.sub(r'<.*?>', '', text) # In case of any HTML\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4152f53",
   "metadata": {},
   "source": [
    "Load SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f0fc92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\terel\\projects\\resume-matcher\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b13844e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the resume categories and job titles for SBERT\n",
    "full_resumes = [\n",
    "        f'{category} {text}' for category, text in zip(resumes['category'], resumes['resume'])\n",
    "]\n",
    "full_jobs = [\n",
    "        f'{title} {description}' for title, description in zip(jobs['job_title'], jobs['job_description'])\n",
    "]\n",
    "\n",
    "# Clean the resume's and jobs\n",
    "cleaned_resumes = [clean_text(r) for r in full_resumes]\n",
    "cleaned_jobs = [clean_text(j) for j in full_jobs]\n",
    "\n",
    "# Get the embeddings\n",
    "resume_embeddings = model.encode(cleaned_resumes, convert_to_tensor=True)\n",
    "job_embeddings = model.encode(cleaned_jobs, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b49f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# Get cosine similarity between each resume and each job\n",
    "similarity_matrix = cos_sim(resume_embeddings, job_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3d4f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 329 - Score: 0.7256\n",
      "Machine Learning\n",
      "\n",
      "Job 1183 - Score: 0.7078\n",
      "Machine Learning\n",
      "\n",
      "Job 1831 - Score: 0.7025\n",
      "Machine Learning\n",
      "\n",
      "Job 1217 - Score: 0.6935\n",
      "Machine Learning\n",
      "\n",
      "Job 1688 - Score: 0.6863\n",
      "Machine Learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "resume_idx = 0\n",
    "similarities = similarity_matrix[resume_idx]\n",
    "\n",
    "top_k = torch.topk(similarities, k=5)\n",
    "top_indices = top_k.indices.tolist()\n",
    "top_scores = top_k.values.tolist()\n",
    "\n",
    "for i, score in zip(top_indices, top_scores):\n",
    "        print(f'Job {i} - Score: {score:.4f}')\n",
    "        print(jobs.iloc[i]['job_title'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a490326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume 0 Title: Data Science\n",
      "\n",
      "Resume 0 Text:\n",
      "Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details \n",
      "\n",
      "Data Science Assurance Associate \n",
      "\n",
      "Data Science Assurance Associate - Ernst & Young LLP\n",
      "Skill Details \n",
      "JAVASCRIPT- Exprience - 24 months\n",
      "jQuery- Exprience - 24 months\n",
      "Python- Exprience - 24 monthsCompany Details \n",
      "company - Ernst & Young LLP\n",
      "description - Fraud Investigations and Dispute Services   Assurance\n",
      "TECHNOLOGY ASSISTED REVIEW\n",
      "TAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\n",
      "* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\n",
      "* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.\n",
      "* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\n",
      "\n",
      "Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\n",
      "\n",
      "MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\n",
      "TEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.\n",
      "* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.\n",
      "* Created customized tableau dashboards for effective reporting and visualizations.\n",
      "CHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.\n",
      "* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.\n",
      "* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.\n",
      "\n",
      "Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\n",
      "\n",
      "INFORMATION GOVERNANCE\n",
      "Organizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.\n",
      "* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\n",
      "* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.\n",
      "* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.\n",
      "Tools & Technologies: Python, Flask, Elastic Search, Kibana\n",
      "\n",
      "FRAUD ANALYTIC PLATFORM\n",
      "Fraud Analytics and investigative platform to review all red flag cases.\n",
      "â¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.\n",
      "* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\n",
      "Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js\n"
     ]
    }
   ],
   "source": [
    "print(\"Resume 0 Title:\", resumes.iloc[0]['category'])\n",
    "print(\"\\nResume 0 Text:\")\n",
    "print(resumes.iloc[0]['resume'])  # or ['cleaned_resume'] if you stored the cleaned version separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70dc085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Job 329 ---\n",
      "Title: Machine Learning\n",
      "Description: Job Description:\n",
      "We are looking for an experienced Data Scientist having excellent working knowledge of NLP & deep learning frameworks. He/She should be keen to collaborate with product and engineering heads using notebooks and visualizations. He/she should ask right questions, connect the dots and uncover hidden potential of data.\n",
      "Responsibilities:\n",
      "· Work as the lead data strategist, identifying and integrating new datasets that can be leveraged through the product capabilities and work closely with the engineering team to strategize and execute the development of data products\n",
      "· Develop advanced algorithms that solve problems of large dimensionality in a computationally efficient and statistically effective manner\n",
      "· Execute application of statistical and data mining techniques (e.g. hypothesis testing, machine learning and retrieval processes) on large, unstructured data sets to identify trends, figures and other relevant information\n",
      "· Evaluate emerging datasets and technologies\n",
      "· Own development of select assets/accelerators that create scale\n",
      "Mandatory skillset:\n",
      "· Deep understanding and experience with Python libraries - Pandas and Numpy in regards to Large dataset processing and Linear Algebra respectively\n",
      "· Excellent understanding of Statistics & Probability using Python\n",
      "· Ability to write, debug and resolve issues in complex SQL scripts\n",
      "· Good experience with at least one NLP library - Spacy, Gensim, NLTK\n",
      "· Good understanding of NLP concepts - Stemming, Lemmatization, NER, TF-IDF, WordNet, Bag of words, Rule based NLP vs Statistical NLP, current challenges in NLP\n",
      "· Good experience with at least one Notebook - Jupyter, Google Collab, AWS sagemaker and comfortable with visualization using matplotlib, seaborn\n",
      "· Exposure to Scikit based machine learning models\n",
      "· Experience with Feature Engineering concepts - Feature Selection, Feature transformation, Feature extraction\n",
      "· Experience with AWS services; Conceptual understanding of AWS Lambda\n",
      "· Experience with distributed training, MLOps - model.joblib, pkl, model deployment, awareness of approach towards shipping trained model to production\n",
      "· Conceptual understanding/POC of Deep learning basics - Neural networks, Tensorflow/Keras, Classification, Regression\n",
      "Optional Skillset:\n",
      "· Transfer Learning using BERT\n",
      "· Experience with Data warehouse like AWS Redshift, Snowflake, Google Bigquery\n",
      "Skills :\n",
      "deep learning frameworks, Scripting, NLP libraries, Testing, Python, Algorithms, aws, SQL\n",
      "Job Type: Full-time\n",
      "Salary: ₹360,121.00 - ₹1,706,410.00 per year\n",
      "Schedule:\n",
      "Day shift\n",
      "Experience:\n",
      "total worNLP library - Spacy, Gensim, NLTKk: 5 years (Preferred)\n",
      "Python libraries - Pandas and Numpy i: 5 years (Preferred)\n",
      "wAWS services; Conceptual understanding of AWS Lambdaork: 5 years (Preferred)\n",
      "Education:\n",
      "Secondary(10th Pass) (Preferred)\n",
      "\n",
      "--- Job 775 ---\n",
      "Title: Machine Learning\n",
      "Description: Job Description:\n",
      "\n",
      "KEY QUALIFICATIONS\n",
      "\n",
      "Solid mathematical foundation in statistics and algebra\n",
      "BS in Computer Science, Statistics, or other data-related field coupled with 4 + years industry experience in machine learning and statistical modelling\n",
      "OR\n",
      "\n",
      "Masterâ€™s Degree in Computer Science, Statistics, or other data-related field coupled with 2+ years industry experience in machine learning and statistical modelling\n",
      "Profound knowledge in machine learning and deep learning techniques\n",
      "Strong coding skills in python\n",
      "Experience working with and creating data structures architectures.\n",
      "Proficiency in at least one major machine learning framework, such as Tensorflow, PyTorch etc.\n",
      "Proven track record of completing multiple data science projects end-to-end; from idea generation, objectives formulation, to implementation and deliverable\n",
      "Experience in developing/implementing machine learning models for Advanced OCR and Cognitive Data extraction capability as well as its execution.\n",
      "Experience visualizing data to partners\n",
      "Creativity and curiosity for solving highly complex problems\n",
      "Ability to work with a â€˜sense of urgencyâ€™ in order to meet critical deadlines\n",
      "Excellent communication and collaboration skills\n",
      "Prior experience in developing algorithms, optimizing /finetuning algorithms for automotive applications is a huge advantage.\n",
      "Bonus - Hands-on production-quality development skills in the Finance domain is an added advantage.\n",
      "Bonus - Contributions to research communities/efforts, including publishing papers in machine learning (JMLR, ICLR, NeurIPS, ICML, ACL, CVPR).\n",
      "Experience with NLP/NLU â€“ developing models/algorithms and improving its results for extracting intents, entities, topic modelling, user-defined entities, multi-languages, sentiment analysis, emotion analysis, spell correction etc.\n",
      "Experience with Neural Networks, NLTK, Spacy, BERT, LSTM, etc.\n",
      "Solid background in statistical learning techniques for NLP (HMMs, CRFs, SVMs, LDA, LSI, MRFs)\n",
      "\n",
      "KEY RESPONSIBILITIES\n",
      "\n",
      "Data gathering, data processing and data mining of large and complex datasets.\n",
      "Pre-process structured and unstructured data\n",
      "Develop processes and tools to analyse model performances\n",
      "Develop, maintain, and deploy NLP/ML Pipeline and models\n",
      "Leads analytical experiments in a methodical manner to find opportunities for product and process optimization.\n",
      "Partners with Data Architects, Data Analysts, Data Engineers and Visualization Experts to develop data-driven solutions for the platform.\n",
      "Mentor and lead team mates to advance our product development goals\n",
      "Interact cross-functionally with a wide variety of people and teams\"\n",
      "00-9.00 Years\n",
      "\n",
      "--- Job 605 ---\n",
      "Title: Machine Learning\n",
      "Description: Job Description\n",
      "\n",
      "We are looking for a Data Scientist that will help us discover the information hidden in vast amounts of data, and help us make smarter decisions to deliver even better products. Your primary focus will be in applying data mining techniques, doing statistical analysis, and building high quality prediction systems integrated with our products.\n",
      "\n",
      "Responsibilities\n",
      "\n",
      "• Selecting features, building and optimizing classifiers using machine learning techniques.\n",
      "• Data mining using state-of-the-art methods.\n",
      "• Extending company’s data with third party sources of information when needed.\n",
      "• Enhancing data collection procedures to include information that is relevant for building analytic systems.\n",
      "• Processing, cleansing, and verifying the integrity of data used for analysis.\n",
      "• Doing ad-hoc analysis and presenting results in a clear manner.\n",
      "• Creating automated anomaly detection systems and constant tracking of its performance.\n",
      "\n",
      "Required Skills\n",
      "\n",
      "• Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.\n",
      "• Experience with common data science toolkits, such as R, Perl, Python, SparkML, Weka, NumPy, MatLab, etc.\n",
      "• Experience with data visualization tools, such as D3.js, GGplot, etc.\n",
      "• Proficiency in using query languages such as SQL, Hive, Pig.\n",
      "• Experience with NoSQL databases, such as MongoDB, Cassandra, HBase.\n",
      "• Experience with Hadoop or similar distributed computing and storage platforms.\n",
      "• Good applied statistics skills, such as distributions, statistical testing, regression, etc.\n",
      "• Good scripting and programming skills.\n",
      "• Exceptional analytical abilities,creativity and attention to details.\n",
      "• Good organizational and problem solving skills.\n",
      "• Good team player who is a self-starter and well organized.\n",
      "• Strong oral and written communication skills.\n",
      "\n",
      "Required Education\n",
      "\n",
      "• Graduate degree in Math, Statistics, Computer Science, or other quantitative discipline.\n",
      "\n",
      "Please email your resume to careers@gtpltech.com\n",
      "\n",
      "--- Job 1688 ---\n",
      "Title: Machine Learning\n",
      "Description: Key Requirements of the Role:\n",
      "Bachelor's degree in a quantitative or related field\n",
      "MS/PhD in a quantitative discipline such as Statistics, Physics, Economics, Applied Math, Computer Science, Operations Research, or Computational Sciences, with coursework and projects in machine learning and data analysis\n",
      "3+ years of related experience\n",
      "Strong understanding of advanced data mining techniques, curating, processing and transforming data to produce sound datasets.\n",
      "Strong understanding of the Machine Learning lifecycle - feature engineering, training, validation, scaling, deployment, scoring, monitoring, and feedback loop\n",
      "Experience in analyzing complex problems and translating it into an analytical approach.\n",
      "Experience in Supervised and Unsupervised Machine Learning including Classification, Forecasting, Anomaly detection, Pattern detection, Text Mining, using variety of techniques such as Decision trees, Time Series Analysis, Bagging and Boosting algorithms, Neural Networks, Deep Learning.\n",
      "Experience with analytical programming languages, tools and libraries (Python ecosystem preferred, but R will be considered)\n",
      "Experience in SQL and relational databases and Big Data technologies e.g. Spark/Hadoop/H2O\n",
      "Good understanding of programming best practices and building for re-use\n",
      "Fostering a collaborative and inclusive environment that supports growth and development\n",
      "Proven ability to work in an extremely fast paced environment, meet deadlines, and perform at high standards with limited supervision.JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n",
      "We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.\n",
      "\n",
      "--- Job 90 ---\n",
      "Title: Machine Learning\n",
      "Description: About Us:\n",
      "CareerNinja is building a personalised & experiential learning platform to make high-quality learning accessible at a mass global scale. It is revolutionising the way youth kickstart & upgrade their career. At the core of it is a unique adaptive learning product with an exclusive technology which enables course creation with 1/10th the effort & cost.\n",
      "*\n",
      "Why Work With Us:\n",
      "Being an education organisation, we take your growth seriously. Working with us could be your best decision if you're looking for learning, wide exposure & looking to be in the driving seat of what you do. You will be given real responsibilities, freedom to make decisions and come up with ideas and work closely with the founder and the core team, all in a flexible, casual and young (everyone under 28) work environment.\n",
      "Work from home(even without Covid), flexible work, free snacks, etc are just a regular part of our work life.\n",
      "Machine Learning Engineer\n",
      "We are looking for an expert in Data Engineering, Machine Learning, Deep Learning and NLP. You will lead all the processes from R&D, data collection, cleaning, and preprocessing, to training models and deploying them to production. The ideal candidate will be passionate about artificial intelligence and stay up-to-date with the latest developments in the field. Most of our work revolves around text data so we need a candidate who has proven expertise in recent advancements in the field of NLP.\n",
      "Responsibilities:\n",
      "Data Collection and Curation.\n",
      "Select appropriate annotated datasets for Supervised Learning methods.\n",
      "Use effective text representations to transform natural language into useful features.\n",
      "Find and implement the right algorithms and tools for NLP tasks.\n",
      "Develop ML/DL/NLP models according to requirements.\n",
      "Train the developed model and run evaluation experiments.\n",
      "Perform statistical analysis of results and refine models.\n",
      "Extend ML libraries and frameworks to apply in NLP tasks.\n",
      "Building recommendation engines to increase the personalization of content for the user.\n",
      "Testing and deployment of models on cloud services i.e. AWS/GCP etc.\n",
      "Act as a bridge between product and engineering team to translate product requirements into the technical solutions.\n",
      "Stay updated and use industry best practices and latest technology trends.\n",
      "Prototype ideas rapidly using cutting edge technologies, drive architecture discussions, and proposes solutions to system and product changes.\n",
      "Requirements:\n",
      "Bachelors or higher degree in Computer Science or related technical field.\n",
      "2+ years of work experience in NLP, Machine Learning, Deep Learning and Data Science.\n",
      "Strong experience in developing, deploying and monitoring large scale models in production environments from scratch.\n",
      "Deep understanding and working knowledge of frameworks like PyTorch/TensorFlow/Keras.\n",
      "Strong Exposure to at least 1-2 state of the art NLP algorithms e.g GPT, BERT, T5, RoBERTa, Hugging Faces etc.\n",
      "Deep understanding of NLP techniques for text representation, semantic extraction techniques, data structures and language modelling.\n",
      "Experience with NLP libraries and technologies including Spacy, NLTK, Gensim, Stanford NLP etc.\n",
      "Proficiency with Python and basic libraries for machine learning such as Numpy, Scikit-learn and pandas.\n",
      "Exposure to DL Model optimisation and transfer learning technique.\n",
      "Experience of cloud environment AWS/GCP, Hadoop/Spark, SQL is a plus.\n",
      "Experience with technologies like Docker, Kubernetes, CI/CD, Git is a plus.\n",
      "Experience with ML Ops is a plus.\n",
      "A team player who can work independently too.\n",
      "Excellent written and verbal communication skills.\n",
      "Multi-tasking and time-management skills, with the ability to prioritize tasks.\n",
      "*\n",
      "Job Type: Full-time\n",
      "Salary: ₹800,000.00 - ₹1,500,000.00 per year\n",
      "Experience:\n",
      "Machine Learning: 2 years (Required)\n",
      "work: 3 years (Preferred)\n",
      "total work: 2 years (Preferred)\n",
      "Education:\n",
      "Bachelor's (Required)\n",
      "Work Remotely:\n",
      "Temporarily due to COVID-19\n"
     ]
    }
   ],
   "source": [
    "job_indices = [329, 775, 605, 1688, 90]\n",
    "\n",
    "for idx in job_indices:\n",
    "    print(f\"\\n--- Job {idx} ---\")\n",
    "    print(\"Title:\", jobs.iloc[idx]['job_title'])\n",
    "    print(\"Description:\", jobs.iloc[idx]['job_description'])  # or ['cleaned_job'] if cleaned\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
